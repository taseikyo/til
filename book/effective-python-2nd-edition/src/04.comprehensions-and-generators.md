# 4.Comprehensions and Generators

## Item 27: Use Comprehensions Instead of map and filter

Python provides compact syntax for deriving a new list from another sequence or iterable -> list comprehensions

```Python
a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
squares = []
for x in a:
	squares.append(x**2)

squares = [x**2 for x in a]

squares = map(lambda x: x**2, a)
```

```Python
even_squares = [x**2 for x in a if x % 2 == 0]
alt = map(lambda x: x**2, filter(lambda x: x % 2 == 0, a))
```

```Python
even_squares_dict = {x: x**2 for x in a if x % 2 == 0}
threes_cubed_set = {x**3 for x in a if x % 3 == 0}

alt_dict = dict(map(lambda x: (x, x**2), filter(lambda x: x % 2 == 0, a)))
alt_set = set(map(lambda x: x**3, filter(lambda x: x % 3 == 0, a)))
```

- List comprehensions are clearer than the map and filter built-in functions because they don’t require lambda expressions
- List comprehensions allow you to easily skip items from the input list, a behavior that map doesn’t support without help from filter
- Dictionaries and sets may also be created using comprehensions

## Item 28: Avoid More Than Two Control Subexpressions in Comprehensions

- Comprehensions support multiple levels of loops and multiple conditions per loop level
- Comprehensions with more than two control subexpressions are very difficult to read and should be avoided

## Item 29: Avoid Repeated Work in Comprehensions by Using Assignment Expressions

```Python
stock = { 'nails': 125, 'screws': 35, 'wingnuts': 8, 'washers': 24,}
order = ['screws', 'wingnuts', 'clips']
def get_batches(count, size):
	return count // size

result = {}
for name in order:
	count = stock.get(name, 0)
	batches = get_batches(count, 8)
	if batches:
		result[name] = batches

print(result)
```

```Python
found = {name: get_batches(stock.get(name, 0), 8)
         for name in order
         if get_batches(stock.get(name, 0), 8)}
print(found)
```

上面两种结果是一样的，问题是下面 `get_batches(stock.get(name, 0), 8)` 重复了，还容易导致问题，比如 get_batches 第二个参数换了，但是前后没同步

```Python
has_bug = {name: get_batches(stock.get(name, 0), 4)
         for name in order
         if get_batches(stock.get(name, 0), 8)}
print(has_bug)
```

An easy solution to these problems is to use the walrus operator (:=), which was introduced in Python 3.8, to form an assignment expression as part of the comprehension

```Python
found = {name: batches for name in order 
		 if (batches := get_batches(stock.get(name, 0), 8))}
```

The assignment expression (batches := get_batches(...)) allows me to look up the value for each order key in the stock dictionary a single time, call get_batches once, and then store its corresponding value in the batches variable. I can then reference that variable elsewhere in the comprehension to construct the dict’s contents instead of having to call get_batches a second time

It’s valid syntax to define an assignment expression in the value expression for a comprehension. But if you try to reference the variable it defines in other parts of the comprehension, you might get an exception at runtime because of the order in which comprehensions are evaluated:

```Python
result = {name: (tenth := count // 10)
		  for name, count in stock.items() if tenth > 0}
>>>
Traceback ...
NameError: name 'tenth' is not defined
```

fix this example by moving the assignment expression into the condition and then referencing the variable name it defined in the comprehension’s value expression

```Python
result = {name: tenth for name, count in stock.items()
		  if (tenth := count // 10) > 0}
print(result)
```

If a comprehension uses the walrus operator in the value part of the comprehension and doesn’t have a condition, it’ll leak the loop variable into the containing scope:

```Python
half = [(last := count // 2) for count in stock.values()]
print(f'Last item of {half} is {last}')

>>>
Last item of [62, 17, 4, 12] is 12
```

This leakage of the loop variable is similar to what happens with a normal for loop:

```Python
for count in stock.values(): # Leaks loop variable
	pass
print(f'Last item of {list(stock.values())} is {count}')
>>>
Last item of [125, 35, 8, 24] is 24
```

However, similar leakage doesn’t happen for the loop variables from comprehensions:

```Python
half = [count // 2 for count in stock.values()]
print(half) # Works
print(count) # Exception because loop variable didn't leak

>>>
[62, 17, 4, 12]
Traceback ...
NameError: name 'count' is not defined
```

It’s better not to leak loop variables, so I recommend using assignment expressions only in the condition part of a comprehension

Using an assignment expression also works the same way in generator expressions:

```Python
found = ((name, batches) for name in order
		  if (batches := get_batches(stock.get(name, 0), 8)))
print(next(found))
print(next(found))

>>>
('screws', 4)
('wingnuts', 1)
```

- Assignment expressions make it possible for comprehensions and generator expressions to reuse the value from one condition elsewhere in the same comprehension, which can improve readability and performance
- Although it’s possible to use an assignment expression outside of a comprehension or generator expression’s condition, you should avoid doing so

## Item 30: Consider Generators Instead of Returning Lists

```Python
def index_words(text):
    result = []
    if text:
        result.append(0)
    for index, letter in enumerate(text):
        if letter == ' ':
            result.append(index + 1)
    return result


address = 'Four score and seven years ago...'
result = index_words(address) print(result[:10])

>>>
[0, 5, 11, 15, 21, 27, 31, 35, 43, 51]
```

```Python
def index_words_iter(text):
    if text:
        yield 0
    for index, letter in enumerate(text):
        if letter == ' ':
            yield index + 1
```

- Using generators can be clearer than the alternative of having a function return a list of accumulated results
- The iterator returned by a generator produces the set of values passed to yield expressions within the generator function’s body
- Generators can produce a sequence of outputs for arbitrarily large inputs because their working memory doesn’t include all inputs and outputs

## Item 31: Be Defensive When Iterating Over Arguments

the `collections.abc` built-in module defines an `Iterator` class that can be used in an `isinstance` test to recognize the potential problem:

```Python
from collections.abc import Iterator

def normalize_defensive(numbers):
	if isinstance(numbers, Iterator): # Another way to check
		raise TypeError('Must supply a container')
	total = sum(numbers)
	result = []
	for value in numbers:
		percent = 100 * value / total
		result.append(percent)
	return result
```

- Beware of functions and methods that iterate over input arguments multiple times. If these arguments are iterators, you may see strange behavior and missing values
- Python’s iterator protocol defines how containers and iterators interact with the iter and next built-in functions, for loops, and related expressions
- You can easily define your own iterable container type by implementing the __iter__ method as a generator
- You can detect that a value is an iterator (instead of a container) if calling iter on it produces the same value as what you passed in. Alternatively, you can use the isinstance built-in function along with the collections.abc.Iterator class

## Item 32: Item 33: Compose Multiple Generators with yield from

```Python
def move(period, speed):
	for _ in range(period):
		yield speed

def pause(delay):
	for _ in range(delay):
		yield 0

def animate():
	for delta in move(4, 5.0):
		yield delta
	for delta in pause(3):
		yield delta
	for delta in move(2, 3.0):
		yield delta

def render(delta):
	print(f'Delta: {delta:.1f}') # Move the images onscreen
	...

def run(func):
	for delta in func():
		render(delta)

run(animate)

>>>
Delta: 5.0 
Delta: 5.0 
Delta: 5.0 
Delta: 5.0 
Delta: 0.0 
Delta: 0.0 
Delta: 0.0 
Delta: 3.0 
Delta: 3.0
```

The problem with this code is the repetitive nature of the `animate` function. The redundancy of the `for` statements and `yield` expressions for each generator adds noise and reduces readability

The solution to this problem is to use the `yield from` expression

This advanced generator feature allows you to yield all values from a nested generator before returning control to the parent generator

```Python
def animate_composed():
	yield from move(4, 5.0)
	yield from pause(3)
	yield from move(2, 3.0)

run(animate_composed)

>>>
Delta: 5.0 
Delta: 5.0 
Delta: 5.0 
Delta: 5.0 
Delta: 0.0 
Delta: 0.0 
Delta: 0.0 
Delta: 3.0 
Delta: 3.0
```

`yield from` essentially causes the Python interpreter to handle the nested for loop and yield expression boilerplate for you, which results in better performance

```Python
import timeit


def child():
    for i in range(1_000_000):
        yield i


def slow():
    for i in child():
        yield i


def fast():
    yield from child()


baseline = timeit.timeit(
	stmt='for _ in slow(): pass',
	globals=globals(),
	number=50)
print(f'Manual nesting {baseline:.2f}s')

comparison = timeit.timeit(
    stmt='for _ in fast(): pass',
    globals=globals(),
    number=50)
print(f'Composed nesting {comparison:.2f}s')

reduction = -(comparison - baseline) / baseline
print(f'{reduction:.1%} less time')

>>>
Manual nesting 4.60s
Composed nesting 3.34s
27.4% less time
```

- The yield from expression allows you to compose multiple nested generators together into a single combined generator
- yield from provides better performance than manually iterating nested generators and yielding their outputs

## Item 34: Avoid Injecting Data into Generators with send

```Python
def my_generator():
	received = yield 1
	print(f'received = {received}')

it = iter(my_generator())
output = next(it) # Get first generator output
print(f'output = {output}')

try:
	next(it)
except StopIteration:
	pass

>>>
output = 1
received = None
```

When I call the `send` method instead of iterating the generator with a `for` loop or the `next` built-in function, the supplied parameter becomes the value of the yield expression when the generator is resumed

However, when the generator first starts, a `yield` expression has not been encountered yet, so the only valid value for calling send initially is `None` (any other argument would raise an exception at runtime):

```python
it = iter(my_generator())
output = it.send(None) # Get first generator output
print(f'output = {output}')
try:
	it.send('hello!') # Send value into the generator
except StopIteration:
	pass

>>>
output = 1
received = hello!
```


```python
def transmit(output):
    if output is None:
        print(f'Output is None')
    else:
        print(f'Output: {output:>5.1f}')


def wave_modulating(steps):
    step_size = 2 * math.pi / steps
    amplitude = yield  # Receive initial amplitude
    for step in range(steps):
        radians = step * step_size
        fraction = math.sin(radians)
        print(f"in: {amplitude}")
        output = amplitude * fraction
        amplitude = yield output  # Receive next amplitude


def run_modulating(it):
    amplitudes = [None, 7, 7, 7, 2, 2, 2, 2, 10, 10, 10, 10, 10]
    for amplitude in amplitudes:
        output = it.send(amplitude)
        transmit(output)

run_modulating(wave_modulating(12))

>>>
Output is None 
Output: 0.0 
Output: 3.5 
Output: 6.1 
Output: 2.0 
Output: 1.7 
Output: 1.0 
Output: 0.0 
Output: -5.0 
Output: -8.7 
Output: -10.0 
Output: -8.7 
Output: -5.0
```

The first output is `None`, as expected, because a value for the amplitude wasn’t received by the generator until after the initial yield expression

```python
def complex_wave_modulating():
	yield from wave_modulating(3)
	yield from wave_modulating(4)
	yield from wave_modulating(5)

run_modulating(complex_wave_modulating())

>>>
Output is None 
Output: 0.0 
Output: 6.1 
Output: -6.1 
Output is None 
Output: 0.0 
Output: 2.0 
Output: 0.0 
Output: -10.0 
Output is None 
Output: 0.0 
Output: 9.5 
Output: 5.9
```

There are many None values in the output! Why does this happen? 

When each yield from expression finishes iterating over a nested generator, it moves on to the next one

Each nested generator starts with a bare yield expression—one without a value—in order to receive the initial amplitude from a generator send method call


My advice is to avoid the `send` method entirely and go with a simpler approach: to pass an iterator into the wave function

The iterator should return an input amplitude each time the next built-in function is called on it

```Markdown
def wave_cascading(amplitude_it, steps):
	step_size = 2 * math.pi / steps
	for step in range(steps):
		radians = step * step_size
		fraction = math.sin(radians)
		amplitude = next(amplitude_it) # Get next input
		output = amplitude * fraction
		yield output

def complex_wave_cascading(amplitude_it):
	yield from wave_cascading(amplitude_it, 3)
	yield from wave_cascading(amplitude_it, 4)
	yield from wave_cascading(amplitude_it, 5)

def run_cascading():
	amplitudes = [7, 7, 7, 2, 2, 2, 2, 10, 10, 10, 10, 10]
	it = complex_wave_cascading(iter(amplitudes))
	for amplitude in amplitudes:
		output = next(it)
		transmit(output)

run_cascading()
```

The best part about this approach is that the iterator can come from anywhere and could be completely dynamic (e.g., implemented using a generator function)

The only downside is that this code assumes that the input generator is completely thread safe, which may not be the case

- The send method can be used to inject data into a generator by giving the yield expression a value that can be assigned to a variable
- Using send with yield from expressions may cause surprising
behavior, such as None values appearing at unexpected times in the generator output
- Providing an input iterator to a set of composed generators is a better approach than using the send method, which should be avoided

## Item 35: Avoid Causing State Transitions in Generators with throw

In addition to yield from expressions and the send method, another advanced generator feature is the throw method for re-raising Exception instances within generator functions

The way throw works is simple: When the method is called, the next occurrence of a yield expression re-raises the provided Exception instance after its output is received instead of continuing normally

```python
class MyError(Exception):
	pass

def my_generator():
	yield 1
	yield 2
	yield 3

it = my_generator()
print(next(it)) # Yield 1
print(next(it)) # Yield 2
print(it.throw(MyError('test error')))

>>> 1 2
Traceback ...
MyError: test error
```

When you call throw, the generator function may catch the injected exception with a standard try/except compound statement that surrounds the last yield expression that was executed

```python
class MyError(Exception):
	pass

def my_generator():
	yield 1
	try:
		yield 2
	except MyError:
		print('Got MyError!')
	else:
		yield 3
	yield 4

it = my_generator()
print(next(it)) # Yield 1
print(next(it)) # Yield 2
print(it.throw(MyError('test error')))

>>>
1
2
Got MyError!
4
```

- The throw method can be used to re-raise exceptions within generators at
the position of the most recently executed yield expression
- Using throw harms readability because it requires additional nesting and boilerplate in order to raise and catch exceptions
- A better way to provide exceptional behavior in generators is to use a class that implements the __iter__ method along with methods to cause exceptional state transitions

## Item 36: Consider itertools for Working with Iterators and Generators

Whenever you find yourself dealing with tricky iteration code, it’s worth looking at the itertools documentation again to see if there’s anything in there for you to use (see help(itertools))

### Linking Iterators Together

- chain

Use chain to combine multiple iterators into a single sequential iterator:

```python
it = itertools.chain([1, 2, 3], [4, 5, 6])
print(list(it))

>>>
[1, 2, 3, 4, 5, 6]
```

- repeat

Use repeat to output a single value forever, or use the second parameter to specify a maximum number of times:

```python
it = itertools.repeat('hello', 3)
print(list(it))

>>>
['hello', 'hello', 'hello']
```

- cycle

Use cycle to repeat an iterator’s items forever:

```python
it = itertools.cycle([1, 2])
result = [next(it) for _ in range (10)]
print(result)

>>>
[1, 2, 1, 2, 1, 2, 1, 2, 1, 2]
```

- tee

Use tee to split a single iterator into the number of parallel iterators specified by the second parameter

The memory usage of this function will grow if the iterators don’t progress at the same speed since buffering will be required to enqueue the pending items:

```python
it1, it2, it3 = itertools.tee(['first', 'second'], 3)
print(list(it1)) 
print(list(it2)) 
print(list(it3))

>>>
['first', 'second']
['first', 'second']
['first', 'second']
```

- zip_longest

This variant of the zip built-in function returns a placeholder value when an iterator is exhausted, which may happen if iterators have different lengths:

```python
keys = ['one', 'two', 'three'] values = [1, 2]
normal = list(zip(keys, values))
print('zip:', normal)
it = itertools.zip_longest(keys, values, fillvalue='nope')
longest = list(it) print('zip_longest:', longest)

>>>
zip: [('one', 1), ('two', 2)]
zip_longest: [('one', 1), ('two', 2), ('three', 'nope')]
```

### Filtering Items from an Iterator

- islice

Use islice to slice an iterator by numerical indexes without copying

You can specify the end, start and end, or start, end, and step sizes, and the behavior is similar to that of standard sequence slicing and striding :

```python
values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
first_five = itertools.islice(values, 5)
print('First five: ', list(first_five))
middle_odds = itertools.islice(values, 2, 8, 2)
print('Middle odds:', list(middle_odds))
>>>
First five: [1, 2, 3, 4, 5]
Middle odds: [3, 5, 7]
```

- takewhile

takewhile returns items from an iterator until a predicate function returns False for an item:

```python
values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
less_than_seven = lambda x: x < 7
it = itertools.takewhile(less_than_seven, values)
print(list(it))

>>>
[1, 2, 3, 4, 5, 6]
```

- dropwhile

dropwhile, which is the opposite of takewhile, skips items from an iterator until the predicate function returns True for the first time:

```python
values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
less_than_seven = lambda x: x < 7
it = itertools.dropwhile(less_than_seven, values)
print(list(it))

>>>
[7, 8, 9, 10]
```

- filterfalse

filterfalse, which is the opposite of the filter built-in function, returns all items from an iterator where a predicate function returns False:

```python
values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
evens = lambda x: x % 2 == 0
filter_result = filter(evens, values)
print('Filter:', list(filter_result))
filter_false_result = itertools.filterfalse(evens, values)
print('Filter false:', list(filter_false_result))

>>>
Filter: [2, 4, 6, 8, 10]
Filter false: [1, 3, 5, 7, 9]
```

### Producing Combinations of Items from Iterators

- accumulate

accumulate folds an item from the iterator into a running value by applying a function that takes two parameters

```python
values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
sum_reduce = itertools.accumulate(values)
print('Sum: ', list(sum_reduce))
def sum_modulo_20(first, second):
	output = first + second
	return output % 20

modulo_reduce = itertools.accumulate(values, sum_modulo_20)
print('Modulo:', list(modulo_reduce))

>>>
Sum: [1, 3, 6, 10, 15, 21, 28, 36, 45, 55]
Modulo: [1, 3, 6, 10, 15, 1, 8, 16, 5, 15]
```

This is essentially the same as the reduce function from the functools built-in module, but with outputs yielded one step at a time

By default it sums the inputs if no binary function is specified

- product

product returns the Cartesian product of items from one or more iterators, which is a nice alternative to using deeply nested list comprehensions:

```python
single = itertools.product([1, 2], repeat=2)
print('Single: ', list(single))
multiple = itertools.product([1, 2], ['a', 'b'])
print('Multiple:', list(multiple))

>>>
Single: [(1, 1), (1, 2), (2, 1), (2, 2)]
Multiple: [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]
```

- permutations

permutations returns the unique ordered permutations of length N with items from an iterator:

```python
it = itertools.permutations([1, 2, 3, 4], 2)
print(list(it))
>>>
[(1, 2), (1, 3), (1, 4), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 4), (4, 1), (4, 2), (4, 3)]
```

- combinations

combinations returns the unordered combinations of length N with unrepeated items from an iterator:

```python
it = itertools.combinations([1, 2, 3, 4], 2)
print(list(it))

>>>
[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]
```

- combinations_with_replacement

combinations_with_replacement is the same as combinations, but repeated values are allowed:

```python
it = itertools.combinations_with_replacement([1, 2, 3, 4], 2)

print(list(it)) >>>
[(1, 1), (1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 3), (3, 4), (4, 4)]
```

- The itertools functions fall into three main categories for working with iterators and generators: linking iterators together, filtering items they output, and producing combinations of items
- There are more advanced functions, additional parameters, and useful recipes available in the documentation at help(itertools)

我：itertools 🐂🍺